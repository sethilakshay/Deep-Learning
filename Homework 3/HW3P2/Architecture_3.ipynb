{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Using LSTMs with CNN based embeddings for Utterances to Phoneme Mapping\n",
        "\n",
        "### Author: Lakshay Sethi\n",
        "#### Custom Architecture 3 (Designed by Lakshay Sethi through Ablations)\n",
        "##### Ablations Link: https://wandb.ai/verydeeplearning/hw3p2-ablations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvyvZt69SjwJ",
        "outputId": "07a96e98-b6e0-47c0-b314-f36daf0bf0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Apr  7 08:26:19 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA9qZoIDcx-h"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "### Levenshtein\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS7a7xeEoaV9"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb --quiet\n",
        "# !pip install python-Levenshtein -q\n",
        "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "# !pip install wget -q\n",
        "# %cd ctcdecode\n",
        "# !pip install . -q\n",
        "# %cd ..\n",
        "\n",
        "# !pip install torchsummaryX -q"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "368afd7b-de11-4034-f8a7-7daa530936bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "933fadb5-044d-4eca-8096-3999e4ae587a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"lakshaysethi\",\"key\":\"32d86595e55bef36c1f649381dc3282f\"}')\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ruxWP60LCQA"
      },
      "outputs": [],
      "source": [
        "# !kaggle competitions download -c 11-785-s23-hw3p2\n",
        "# '''\n",
        "# This will take a couple minutes, but you should see at least the following:\n",
        "# 11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "# '''\n",
        "# !unzip -q 11-785-s23-hw3p2.zip\n",
        "# !ls"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wQbPQj4Yl1z",
        "outputId": "560e6ca4-68fe-45ed-a5d4-e8a3d739a476"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[SIL]',\n",
              " 'NG',\n",
              " 'F',\n",
              " 'M',\n",
              " 'AE',\n",
              " 'R',\n",
              " 'UW',\n",
              " 'N',\n",
              " 'IY',\n",
              " 'AW',\n",
              " 'V',\n",
              " 'UH',\n",
              " 'OW',\n",
              " 'AA',\n",
              " 'ER',\n",
              " 'HH',\n",
              " 'Z',\n",
              " 'K',\n",
              " 'CH',\n",
              " 'W',\n",
              " 'EY',\n",
              " 'ZH',\n",
              " 'T',\n",
              " 'EH',\n",
              " 'Y',\n",
              " 'AH',\n",
              " 'B',\n",
              " 'P',\n",
              " 'TH',\n",
              " 'DH',\n",
              " 'AO',\n",
              " 'G',\n",
              " 'L',\n",
              " 'JH',\n",
              " 'OY',\n",
              " 'SH',\n",
              " 'D',\n",
              " 'AY',\n",
              " 'S',\n",
              " 'IH']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PHONEMES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjT1NzNRYoZ0",
        "outputId": "3aaf5c1b-05b1-4dec-fe51-3a999f63874d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' ',\n",
              " '-',\n",
              " 'G',\n",
              " 'f',\n",
              " 'm',\n",
              " '@',\n",
              " 'r',\n",
              " 'u',\n",
              " 'n',\n",
              " 'i',\n",
              " 'W',\n",
              " 'v',\n",
              " 'U',\n",
              " 'o',\n",
              " 'a',\n",
              " 'R',\n",
              " 'h',\n",
              " 'z',\n",
              " 'k',\n",
              " 'C',\n",
              " 'w',\n",
              " 'e',\n",
              " 'Z',\n",
              " 't',\n",
              " 'E',\n",
              " 'y',\n",
              " 'A',\n",
              " 'b',\n",
              " 'p',\n",
              " 'T',\n",
              " 'D',\n",
              " 'c',\n",
              " 'g',\n",
              " 'l',\n",
              " 'j',\n",
              " 'O',\n",
              " 'S',\n",
              " 'd',\n",
              " 'Y',\n",
              " 's',\n",
              " 'I',\n",
              " '[SOS]',\n",
              " '[EOS]']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ARPAbet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, partition = ['train-clean-100']):\n",
        "        self.mfcc_files, self.transcript_files = [], []\n",
        "\n",
        "        for part in partition:  \n",
        "            mfcc_dir = root + part + '/mfcc/'\n",
        "            transcript_dir = root + part +'/transcript/'\n",
        "\n",
        "            mfcc_temp_dir = os.listdir(mfcc_dir)\n",
        "            mfcc_temp_dir.sort()\n",
        "            mfcc_temp = [os.path.join(mfcc_dir, currFile) for currFile in mfcc_temp_dir]\n",
        "            self.mfcc_files.extend(mfcc_temp)\n",
        "\n",
        "            transcript_temp_dir = os.listdir(transcript_dir)\n",
        "            transcript_temp_dir.sort()\n",
        "            transcript_temp = [os.path.join(transcript_dir, currFile) for currFile in transcript_temp_dir]\n",
        "            self.transcript_files.extend(transcript_temp)\n",
        "\n",
        "        \n",
        "        # Sanity Check\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files)\n",
        "\n",
        "        # Lenght Assignment\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        self.PHONEMES = phonemes\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "        self.PHONEMES_DICT = {value: idx for idx, value in enumerate(self.PHONEMES)}\n",
        "\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        for i in range(self.length):\n",
        "            mfcc = np.load(self.mfcc_files[i], allow_pickle=True)\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)\n",
        "            transcript = np.load(self.transcript_files[i], allow_pickle=True)\n",
        "            transcript = transcript[1:-1]         # Removing [SOS] and [EOS] from the transcript \n",
        "\n",
        "            label = np.array([self.PHONEMES_DICT[trans] for trans in transcript])\n",
        "\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(label)\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc_tensor = torch.FloatTensor(self.mfccs[ind])\n",
        "        transcript_tensor = torch.tensor(self.transcripts[ind])\n",
        "\n",
        "        return mfcc_tensor, transcript_tensor\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        batch_mfcc       = [mfcc for mfcc, _ in batch]\n",
        "        batch_transcript = [transcript for _, transcript in batch]\n",
        "\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=0.0)\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=0.0)\n",
        "        lengths_transcript = [len(trans) for trans in batch_transcript]\n",
        "\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EzAmXxHjTcL"
      },
      "outputs": [],
      "source": [
        "# class customCollate:\n",
        "\n",
        "#     def __init__(self, trainData = True):\n",
        "#         self.trainData = trainData\n",
        "\n",
        "#     def __call__(self, batch):\n",
        "#         batch_mfcc       = [mfcc for mfcc, _ in batch]\n",
        "#         batch_transcript = [transcript for _, transcript in batch]\n",
        "\n",
        "#         if self.trainData:\n",
        "#             time_masking = torchaudio.transforms.TimeMasking(time_mask_param = 10, iid_masks = False, p = 1.0)\n",
        "#             frequency_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param = 20, iid_masks = False)\n",
        "            \n",
        "#             batch_mfcc = \n",
        "\n",
        "#         batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=0.0)\n",
        "#         lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n",
        "\n",
        "#         batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=0.0)\n",
        "#         lengths_transcript = [len(trans) for trans in batch_transcript]\n",
        "\n",
        "#         return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, partition = 'test-clean'): \n",
        "\n",
        "        self.mfcc_dir = root + partition + '/mfcc/'\n",
        "        self.mfcc_files = os.listdir(self.mfcc_dir)\n",
        "        self.mfcc_files.sort()\n",
        "\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        self.mfccs = []\n",
        "\n",
        "        for i in range(self.length):\n",
        "            mfcc = np.load(self.mfcc_dir + self.mfcc_files[i], allow_pickle=True)\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)\n",
        "            self.mfccs.append(mfcc)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        batch_mfcc = batch\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=0)\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n",
        "\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 240 # Increase if your device can handle it\n",
        "\n",
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "root = '/content/hw3p2' "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "90614b22-39dd-40e1-a45c-1540efbb3636"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4OFOjOYNJzu"
      },
      "outputs": [],
      "source": [
        "# del train_data, val_data, test_data, train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "becd4708-92e5-4bdc-804f-33a48f3d95c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  240\n",
            "Train dataset samples = 132552, batches = 553\n",
            "Val dataset samples = 2703, batches = 12\n",
            "Test dataset samples = 2620, batches = 11\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(root = '/content/11-785-s23-hw3p2/', phonemes = PHONEMES, partition = ['train-clean-100', 'train-clean-360'])\n",
        "val_data =AudioDataset(root = '/content/11-785-s23-hw3p2/', phonemes = PHONEMES, partition = ['dev-clean'])\n",
        "test_data = AudioDatasetTest(root = '/content/11-785-s23-hw3p2/', partition = 'test-clean')\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 4,\n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn  = train_data.collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn  = val_data.collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data, \n",
        "    num_workers = 2, \n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True, \n",
        "    shuffle     = False,\n",
        "    collate_fn  = test_data.collate_fn\n",
        ")\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7qZ-LkqMTLP",
        "outputId": "c96cc8a3-1c27-4be1-997e-2d579a3213b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1980"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfbs2gn3lQtA",
        "outputId": "bb861eaa-bb31-4670-9bd8-f69a719ab010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([240, 1679, 27]) torch.Size([240, 233]) torch.Size([240]) torch.Size([240])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M3rabX_-Xfm",
        "outputId": "7e89f654-24f2-4734-a689-b9393015a4d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240\n",
            "240\n"
          ]
        }
      ],
      "source": [
        "print(len(x))\n",
        "print(len(lx))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic\n",
        "\n",
        "This is a basic block for understanding, you can skip this and move to pBLSTM one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# class Network(nn.Module):\n",
        "\n",
        "#     def __init__(self):\n",
        "\n",
        "#         super(Network, self).__init__()\n",
        "\n",
        "#         # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "#         self.embedding = nn.Sequential(nn.Conv1d(27, 512, kernel_size = 9, stride = 1, padding = 4, bias = False),\n",
        "#                                        nn.GELU(),\n",
        "#                                        nn.BatchNorm1d(512),\n",
        "                                       \n",
        "#                                        nn.Conv1d(512, 512, kernel_size = 7, stride = 1, padding = 3, bias = False),\n",
        "#                                        nn.GELU(),\n",
        "#                                        nn.BatchNorm1d(512),\n",
        "\n",
        "#                                        nn.Conv1d(512, 512, kernel_size = 5, stride = 1, padding = 2, bias = False),\n",
        "#                                        nn.GELU(),\n",
        "#                                        nn.BatchNorm1d(512),\n",
        "\n",
        "#                                        nn.Conv1d(512, 512, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
        "#                                        nn.BatchNorm1d(512)\n",
        "#         )\n",
        "\n",
        "#         self.out_features = len(PHONEMES)\n",
        "        \n",
        "#         # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "#         self.lstm = nn.LSTM(input_size = 512, hidden_size = 384, num_layers = 5, bias = True, bidirectional = True, dropout = 0.1) \n",
        "       \n",
        "#         self.classification = nn.Sequential(\n",
        "#             nn.Dropout(p = 0.1, inplace=False),\n",
        "#             torch.nn.Linear(384*2, 384),\n",
        "#             torch.nn.Linear(384, self.out_features)\n",
        "#         )\n",
        "\n",
        "#         self.logSoftmax = torch.nn.LogSoftmax(dim = 2)\n",
        "\n",
        "#     def forward(self, x, lx):\n",
        "#         x = self.embedding(torch.transpose(x, 1, 2))\n",
        "#         x = torch.transpose(x, 1, 2)\n",
        "\n",
        "#         x = pack_padded_sequence(x, lx, batch_first=True, enforce_sorted=False)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         del _\n",
        "\n",
        "#         x, lens_unpacked  = pad_packed_sequence(x, batch_first=True)\n",
        "#         x = self.classification(x)\n",
        "#         x = F.log_softmax(x, dim=2)\n",
        "\n",
        "#         return x, lens_unpacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwp9vVyZZF3J"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiIuFoB0X6uE"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "        self.embedding = nn.Sequential(PermuteBlock(),\n",
        "                                       nn.Conv1d(27, 128, kernel_size = 3, stride = 2, padding = 1, bias = False),\n",
        "                                       nn.Dropout(p = 0.3, inplace = False),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.BatchNorm1d(128),\n",
        "                                       \n",
        "                                       nn.Conv1d(128, 256, kernel_size = 3, stride = 2, padding = 1, bias = False),\n",
        "                                       nn.Dropout(p = 0.3, inplace = False),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.BatchNorm1d(256),\n",
        "                                       PermuteBlock()\n",
        "        )\n",
        "\n",
        "        self.out_features = len(PHONEMES)\n",
        "        \n",
        "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "        self.lstm1 = nn.LSTM(input_size = 256, hidden_size = 384, num_layers = 3, bias = True, bidirectional = True, dropout = 0.2)\n",
        "        self.lstm2 = nn.LSTM(input_size = 768, hidden_size = 768, num_layers = 2, bias = True, bidirectional = True, dropout = 0.2)\n",
        "       \n",
        "        self.classification = nn.Sequential(\n",
        "            torch.nn.Linear(1536, 1536),\n",
        "            nn.Dropout(p = 0.2, inplace = False),\n",
        "            torch.nn.Linear(1536, self.out_features)\n",
        "        )\n",
        "\n",
        "        self.logSoftmax = torch.nn.LogSoftmax(dim = 2)\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        lx = (lx-2)//4\n",
        "        x = pack_padded_sequence(x, lx, batch_first=True, enforce_sorted=False)\n",
        "        x, _ = self.lstm1(x)\n",
        "        del _\n",
        "        x, _ = self.lstm2(x)\n",
        "        del _\n",
        "\n",
        "        x, lens_unpacked  = pad_packed_sequence(x, batch_first=True)\n",
        "        x = self.classification(x)\n",
        "        x = F.log_softmax(x, dim=2)\n",
        "\n",
        "        return x, lens_unpacked"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "# INIT\n",
        "(If trying out the basic Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CGoiXd70tb5z",
        "outputId": "9d4289f5-e600-437a-f8b9-0ad2564293ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================\n",
            "                              Kernel Shape      Output Shape      Params  \\\n",
            "Layer                                                                      \n",
            "0_embedding.PermuteBlock_0               -   [240, 27, 1679]           -   \n",
            "1_embedding.Conv1d_1          [27, 128, 3]   [240, 128, 840]     10.368k   \n",
            "2_embedding.Dropout_2                    -   [240, 128, 840]           -   \n",
            "3_embedding.GELU_3                       -   [240, 128, 840]           -   \n",
            "4_embedding.BatchNorm1d_4            [128]   [240, 128, 840]       256.0   \n",
            "5_embedding.Conv1d_5         [128, 256, 3]   [240, 256, 420]     98.304k   \n",
            "6_embedding.Dropout_6                    -   [240, 256, 420]           -   \n",
            "7_embedding.GELU_7                       -   [240, 256, 420]           -   \n",
            "8_embedding.BatchNorm1d_8            [256]   [240, 256, 420]       512.0   \n",
            "9_embedding.PermuteBlock_9               -   [240, 420, 256]           -   \n",
            "10_lstm1                                 -      [75977, 768]     9.0624M   \n",
            "11_lstm2                                 -     [75977, 1536]  23.617536M   \n",
            "12_classification.Linear_0    [1536, 1536]  [240, 419, 1536]   2.360832M   \n",
            "13_classification.Dropout_1              -  [240, 419, 1536]           -   \n",
            "14_classification.Linear_2      [1536, 41]    [240, 419, 41]     63.017k   \n",
            "\n",
            "                             Mult-Adds  \n",
            "Layer                                   \n",
            "0_embedding.PermuteBlock_0           -  \n",
            "1_embedding.Conv1d_1          8.70912M  \n",
            "2_embedding.Dropout_2                -  \n",
            "3_embedding.GELU_3                   -  \n",
            "4_embedding.BatchNorm1d_4        128.0  \n",
            "5_embedding.Conv1d_5         41.28768M  \n",
            "6_embedding.Dropout_6                -  \n",
            "7_embedding.GELU_7                   -  \n",
            "8_embedding.BatchNorm1d_8        256.0  \n",
            "9_embedding.PermuteBlock_9           -  \n",
            "10_lstm1                     9.043968M  \n",
            "11_lstm2                     23.59296M  \n",
            "12_classification.Linear_0   2.359296M  \n",
            "13_classification.Dropout_1          -  \n",
            "14_classification.Linear_2     62.976k  \n",
            "------------------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          35.213225M\n",
            "Trainable params      35.213225M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             85.056384M\n",
            "====================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-46e914c7-c5ee-4f0d-a3f8-65b0a68dc219\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedding.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[240, 27, 1679]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_embedding.Conv1d_1</th>\n",
              "      <td>[27, 128, 3]</td>\n",
              "      <td>[240, 128, 840]</td>\n",
              "      <td>10368.0</td>\n",
              "      <td>8709120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_embedding.Dropout_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[240, 128, 840]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_embedding.GELU_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[240, 128, 840]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_embedding.BatchNorm1d_4</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[240, 128, 840]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_embedding.Conv1d_5</th>\n",
              "      <td>[128, 256, 3]</td>\n",
              "      <td>[240, 256, 420]</td>\n",
              "      <td>98304.0</td>\n",
              "      <td>41287680.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_embedding.Dropout_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[240, 256, 420]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_embedding.GELU_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[240, 256, 420]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_embedding.BatchNorm1d_8</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[240, 256, 420]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_embedding.PermuteBlock_9</th>\n",
              "      <td>-</td>\n",
              "      <td>[240, 420, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_lstm1</th>\n",
              "      <td>-</td>\n",
              "      <td>[75977, 768]</td>\n",
              "      <td>9062400.0</td>\n",
              "      <td>9043968.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_lstm2</th>\n",
              "      <td>-</td>\n",
              "      <td>[75977, 1536]</td>\n",
              "      <td>23617536.0</td>\n",
              "      <td>23592960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_classification.Linear_0</th>\n",
              "      <td>[1536, 1536]</td>\n",
              "      <td>[240, 419, 1536]</td>\n",
              "      <td>2360832.0</td>\n",
              "      <td>2359296.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_classification.Dropout_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[240, 419, 1536]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_classification.Linear_2</th>\n",
              "      <td>[1536, 41]</td>\n",
              "      <td>[240, 419, 41]</td>\n",
              "      <td>63017.0</td>\n",
              "      <td>62976.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46e914c7-c5ee-4f0d-a3f8-65b0a68dc219')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46e914c7-c5ee-4f0d-a3f8-65b0a68dc219 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46e914c7-c5ee-4f0d-a3f8-65b0a68dc219');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                              Kernel Shape      Output Shape      Params  \\\n",
              "Layer                                                                      \n",
              "0_embedding.PermuteBlock_0               -   [240, 27, 1679]         NaN   \n",
              "1_embedding.Conv1d_1          [27, 128, 3]   [240, 128, 840]     10368.0   \n",
              "2_embedding.Dropout_2                    -   [240, 128, 840]         NaN   \n",
              "3_embedding.GELU_3                       -   [240, 128, 840]         NaN   \n",
              "4_embedding.BatchNorm1d_4            [128]   [240, 128, 840]       256.0   \n",
              "5_embedding.Conv1d_5         [128, 256, 3]   [240, 256, 420]     98304.0   \n",
              "6_embedding.Dropout_6                    -   [240, 256, 420]         NaN   \n",
              "7_embedding.GELU_7                       -   [240, 256, 420]         NaN   \n",
              "8_embedding.BatchNorm1d_8            [256]   [240, 256, 420]       512.0   \n",
              "9_embedding.PermuteBlock_9               -   [240, 420, 256]         NaN   \n",
              "10_lstm1                                 -      [75977, 768]   9062400.0   \n",
              "11_lstm2                                 -     [75977, 1536]  23617536.0   \n",
              "12_classification.Linear_0    [1536, 1536]  [240, 419, 1536]   2360832.0   \n",
              "13_classification.Dropout_1              -  [240, 419, 1536]         NaN   \n",
              "14_classification.Linear_2      [1536, 41]    [240, 419, 41]     63017.0   \n",
              "\n",
              "                              Mult-Adds  \n",
              "Layer                                    \n",
              "0_embedding.PermuteBlock_0          NaN  \n",
              "1_embedding.Conv1d_1          8709120.0  \n",
              "2_embedding.Dropout_2               NaN  \n",
              "3_embedding.GELU_3                  NaN  \n",
              "4_embedding.BatchNorm1d_4         128.0  \n",
              "5_embedding.Conv1d_5         41287680.0  \n",
              "6_embedding.Dropout_6               NaN  \n",
              "7_embedding.GELU_7                  NaN  \n",
              "8_embedding.BatchNorm1d_8         256.0  \n",
              "9_embedding.PermuteBlock_9          NaN  \n",
              "10_lstm1                      9043968.0  \n",
              "11_lstm2                     23592960.0  \n",
              "12_classification.Linear_0    2359296.0  \n",
              "13_classification.Dropout_1         NaN  \n",
              "14_classification.Linear_2      62976.0  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Network().to(device)\n",
        "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S0ql3YJDDJOW"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWG6EOIsHyOY"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vzWp3jN7YYOo"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "## Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# # Utils for network\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# class PermuteBlock(torch.nn.Module):\n",
        "#     def forward(self, x):\n",
        "#         return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "# class pBLSTM(torch.nn.Module):\n",
        "\n",
        "#     '''\n",
        "#     Pyramidal BiLSTM\n",
        "#     Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "#     At each step,\n",
        "#     1. Pad your input if it is packed (Unpack it)\n",
        "#     2. Reduce the input length dimension by concatenating feature dimension\n",
        "#         (Tip: Write down the shapes and understand)\n",
        "#         (i) How should  you deal with odd/even length input? \n",
        "#         (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "#     3. Pack your input\n",
        "#     4. Pass it into LSTM layer\n",
        "\n",
        "#     To make our implementation modular, we pass 1 layer at a time.\n",
        "#     '''\n",
        "    \n",
        "#     def __init__(self, input_size, hidden_size):\n",
        "#         super(pBLSTM, self).__init__()\n",
        "\n",
        "#         self.blstm = # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "\n",
        "#     def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "#         # TODO: Pad Packed Sequence\n",
        "        \n",
        "#         # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "#         # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "#         # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "#         # TODO: Pass the sequence through bLSTM\n",
        "\n",
        "#         # What do you return?\n",
        "\n",
        "#         return NotImplemented\n",
        "\n",
        "#     def trunc_reshape(self, x, x_lens): \n",
        "#         # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "#         # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "#         # TODO: Reduce lengths by the same downsampling factor\n",
        "#         return x, x_lens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "# class Encoder(torch.nn.Module):\n",
        "#     '''\n",
        "#     The Encoder takes utterances as inputs and returns latent feature representations\n",
        "#     '''\n",
        "#     def __init__(self, input_size, encoder_hidden_size):\n",
        "#         super(Encoder, self).__init__()\n",
        "\n",
        "        \n",
        "#         self.embedding = #TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
        "\n",
        "#         self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
        "#             # TODO: Fill this up with pBLSTMs - What should the input_size be? \n",
        "#             # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "#             # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
        "#             # https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n",
        "#             # ...\n",
        "#             # ...\n",
        "#         )\n",
        "         \n",
        "#     def forward(self, x, x_lens):\n",
        "#         # Where are x and x_lens coming from? The dataloader\n",
        "#         #TODO: Call the embedding layer\n",
        "#         # TODO: Pack Padded Sequence\n",
        "#         # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "#         # TODO: Pad Packed Sequence\n",
        "        \n",
        "\n",
        "#         # Remember the number of output(s) each function returns\n",
        "\n",
        "#         return encoder_outputs, encoder_lens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "# class Decoder(torch.nn.Module):\n",
        "\n",
        "#     def __init__(self, embed_size, output_size= 41):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.mlp = torch.nn.Sequential(\n",
        "#             PermuteBlock(), torch.nn.BatchNorm1d(embed_size), PermuteBlock(),\n",
        "#             #TODO define your MLP arch. Refer HW1P2\n",
        "#             #Use Permute Block before and after BatchNorm1d() to match the size\n",
        "#         )\n",
        "        \n",
        "#         self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "#     def forward(self, encoder_out):\n",
        "#         #TODO call your MLP\n",
        "#         #TODO Think what should be the final output of the decoder for the classification \n",
        "\n",
        "#         return out "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "# class ASRModel(torch.nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.augmentations  = torch.nn.Sequential(\n",
        "#             #TODO Add Time Masking/ Frequency Masking\n",
        "#             #Hint: See how to use PermuteBlock() function defined above\n",
        "#         )\n",
        "#         self.encoder        =# TODO: Initialize Encoder\n",
        "#         self.decoder        =# TODO: Initialize Decoder \n",
        "\n",
        "        \n",
        "    \n",
        "#     def forward(self, x, lengths_x):\n",
        "        \n",
        "#         if self.training:\n",
        "#             x = self.augmentations(x)\n",
        "\n",
        "#         encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
        "#         decoder_out                 = self.decoder(encoder_out)\n",
        "\n",
        "#         return decoder_out, encoder_lens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## INIT ASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaaDsnnLMUz2"
      },
      "outputs": [],
      "source": [
        "# model = ASRModel(\n",
        "#     input_size  = 27,\n",
        "#     embed_size  = #TODO\n",
        "#     output_size = len(PHONEMES)\n",
        "# ).to(device)\n",
        "# print(model)\n",
        "# torchsummaryX.summary(model, x.to(device), lx)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"beam_width\" : 5,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 50\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CTCLoss()\n",
        "# Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr = config['lr'], weight_decay = config['lr']/100)\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(LABELS, beam_width = config['beam_width'], log_probs_input=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = config['epochs'], eta_min = config['lr']/200, verbose=True)\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "## Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVJkWRsb1qMp"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP = LABELS):\n",
        "    \n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings    = []\n",
        "    \n",
        "    for i in range(beam_results.shape[0]):\n",
        "        #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "        beam_temp = beam_results[i,0,:out_seq_len[i,0]]\n",
        "        pred_str = \"\".join([PHONEME_MAP[i] for i in beam_temp])\n",
        "        pred_strings.append(pred_str)\n",
        "    \n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP = LABELS): # y - sequence of integers\n",
        "    \n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        # TODO: Get predicted string and label string for each element in the batch\n",
        "        pred_str = pred_strings[i]\n",
        "        label_temp = label[i,0:label_lens[i]]\n",
        "        label_str = [PHONEME_MAP[k] for k in label_temp]\n",
        "        dist += Levenshtein.distance(pred_str, label_str)\n",
        "\n",
        "    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    # raise NotImplemented\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GnTLL-5gMBrY"
      },
      "outputs": [],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PiDduMaDIARE"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"2a3537b39181a31bafb5eabede64d932c94e54e8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4s52yBOvICPZ"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"RNN_HW3_v2\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LGrEFE38MUz5"
      },
      "outputs": [],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = 'Epoch_checkpoint_v2.pth'\n",
        "best_model_path = 'Best_checkpoint_v2.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JR43E28rM9Ak"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(25, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "    \n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    print('Current Learning rate: ', curr_lr)\n",
        "\n",
        "    train_loss              = train_model(model, train_loader, criterion, optimizer)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder, LABELS)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    scheduler.step()\n",
        "    #scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'valid_dist': valid_dist, \n",
        "        'valid_loss': valid_loss, \n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "#run.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "outputs": [],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = int(config['beam_width']*1.5)\n",
        "\n",
        "test_decoder    = CTCBeamDecoder(LABELS, beam_width = TEST_BEAM_WIDTH, log_probs_input=True)\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string = decode_prediction(h, lh, test_decoder, LABELS)\n",
        "    results.extend(prediction_string)\n",
        "    \n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wfEyqWdD1IGe"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/11-785-s23-hw3p2\" + \"/test-clean/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results\n",
        "df.to_csv('submission.csv', index = False)\n",
        "\n",
        "!kaggle competitions submit -c 11-785-s23-hw3p2 -f submission.csv -m \"I made it!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqCVDXkUC-fP"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PB6eh3gnMUzy",
        "g3ZQ75OcMUz0",
        "kg82HXa3MUz1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
