{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ERgBpbcMmB"
      },
      "source": [
        "# MLPs for Frame-Level Speech Recognition\n",
        "\n",
        "### Author: Lakshay Sethi\n",
        "#### Custom Architecture (Designed by Lakshay Sethi through Ablations)\n",
        "##### Ablations Link: https://wandb.ai/highcutoff/Lakshay?workspace=user-sethilakshay13"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkH6GMGcWcE"
      },
      "source": [
        "Using MFCC data consisting of 27 features at each time step/frame to accurately identify the phoneme."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwlC_EPOODqZ",
        "outputId": "1a89c80f-7662-4941-9eb6-cdda10b1d8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Feb 16 09:53:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwYu9sSUnSho",
        "outputId": "805ab381-3374-44a7-f37d-81b80f8d3e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: speechpy in /usr/local/lib/python3.8/dist-packages (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from speechpy) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from speechpy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX wandb --quiet\n",
        "!pip install speechpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "9cd156d8-9630-48c1-fde2-342f78f37080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "import sklearn\n",
        "import speechpy\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "import gc\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yBgXjKV1O0Z"
      },
      "outputs": [],
      "source": [
        "### If you are using colab, you can import google drive to save model checkpoints in a folder\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-9qE20hmCgQ"
      },
      "outputs": [],
      "source": [
        "### PHONEME LIST\n",
        "PHONEMES = [\n",
        "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH']\n",
        "\n",
        "PHONEMES_DICT = {\n",
        "            '[SIL]':0,  'AA':1,     'AE':2,     'AH':3,     'AO':4,     'AW':5,     'AY':6,  \n",
        "            'B':7,      'CH':8,     'D':9,      'DH':10,    'EH':11,    'ER':12,    'EY':13,\n",
        "            'F':14,     'G':15,     'HH':16,    'IH':17,    'IY':18,    'JH':19,    'K':20,\n",
        "            'L':21,     'M':22,     'N':23,     'NG':24,    'OW':25,    'OY':26,    'P':27,\n",
        "            'R':28,     'S':29,     'SH':30,    'T':31,     'TH':32,    'UH':33,    'UW':34,\n",
        "            'V':35,     'W':36,     'Y':37,     'Z':38,     'ZH':39}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BBCbeRhixGM7"
      },
      "source": [
        "This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPBUd7Cnl-Rx",
        "outputId": "10cbb9cc-18cf-4481-cc83-696413448464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"lakshaysethi\",\"key\":\"ad796e545fa89ae311c2e0f59e4ea1a0\"}') \n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if2Somqfbje1"
      },
      "outputs": [],
      "source": [
        "# commands to download data from kaggle\n",
        "\n",
        "# !kaggle competitions download -c 11-785-s23-hw1p2\n",
        "# !mkdir '/content/data'\n",
        "\n",
        "# !unzip -qo '11-785-s23-hw1p2.zip' -d '/content/data'\n",
        "# !rm -rf '11-785-s23-hw1p2.zip'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuzce0_TdcaR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7QgMbBdgPp"
      },
      "source": [
        "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
        "\n",
        "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpLCvi3AJC5z"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load train and validation data\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, context=0, partition= ['train-clean-100'], ablations = False, cutOff = 1.0): # Feel free to add more arguments\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        \n",
        "        for idx in range(len(partition)):\n",
        "            \n",
        "            self.mfcc_dir       = root + partition[idx] + '/mfcc/'   # Accessing the mfcc directory\n",
        "            self.transcript_dir = root + partition[idx] + '/transcript/'  # Accessing the transcript directory\n",
        "\n",
        "            mfcc_names          = os.listdir(self.mfcc_dir) #List files in self.mfcc_dir using os.listdir \n",
        "            mfcc_names.sort()\n",
        "\n",
        "            transcript_names    = os.listdir(self.transcript_dir)   #List files in self.transcript_dir using os.listdir \n",
        "            transcript_names.sort()\n",
        "\n",
        "            assert len(mfcc_names) == len(transcript_names) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "            for i in range(len(mfcc_names)):    #Iterating through mfccs and transcripts\n",
        "            \n",
        "                mfcc        = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle=True) #   Load a single mfcc\n",
        "                mfcc        = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)   #   Do Cepstral Normalization of mfcc\n",
        "\n",
        "                transcript  = np.load(self.transcript_dir + transcript_names[i], allow_pickle=True) #   Load the transcript\n",
        "                transcript = transcript[1:-1]   # Remove [SOS] and [EOS] from the transcript. SOS will always be in the starting and EOS at end\n",
        "                \n",
        "            #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "                self.mfccs.append(mfcc)\n",
        "                self.transcripts.append(transcript)\n",
        "                \n",
        "            ##  Part of code to restrict the ablations train dataset size to 20%\n",
        "                if((ablations) and (i >= cutOff*len(mfcc_names))):\n",
        "                    break\n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
        "        # Each transcript is of shape (T1+2) x 27, (T2+2) x 27 before removing [SOS] and [EOS]\n",
        "\n",
        "        # Concatenating self.mfcc final shape is T x 27 (Where T = T1 + T2 + ...) \n",
        "        self.mfccs          = np.concatenate(self.mfccs, axis = 0)\n",
        "\n",
        "        # Concatenating self.transcripts the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        self.transcripts    = np.concatenate(self.transcripts, axis = 0)\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # Context Padding\n",
        "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), mode = 'constant', constant_values = 0)\n",
        "\n",
        "        # The available phonemes in the transcript are of string data type\n",
        "        # But the neural network cannot predict strings as such. \n",
        "        # Hence, we map these phonemes to integers\n",
        "\n",
        "        # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n",
        "        self.transcripts = np.array([PHONEMES_DICT[trans] for trans in self.transcripts])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind: ind+2*self.context+1]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten()\n",
        "\n",
        "        # Converting to Tensors\n",
        "        frames      = torch.FloatTensor(frames)\n",
        "        phonemes    = torch.tensor(self.transcripts[ind])       \n",
        "\n",
        "        return frames, phonemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm6ISwPBh-M7"
      },
      "outputs": [],
      "source": [
        "# Dataset class to only load train Data\n",
        "\n",
        "class AudioDatasetEfficientTrain(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, phonemes_dict = PHONEMES_DICT, context = 0, partition = ['train-clean-100']):\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "        self.masked     = []\n",
        "    \n",
        "        ############ Efficient Data Loader ###############\n",
        "        ### Need to know the shape first\n",
        "        T = 0\n",
        "        for idx in range(len(partition)):\n",
        "        \n",
        "            self.mfcc_dir       = root + partition[idx] + '/mfcc/'   # Accessing the mfcc directory\n",
        "            self.transcript_dir = root + partition[idx] + '/transcript/'  # Accessing the transcript directory\n",
        "\n",
        "            mfcc_names          = os.listdir(self.mfcc_dir) #List files in self.mfcc_dir using os.listdir \n",
        "            mfcc_names.sort()\n",
        "\n",
        "            transcript_names    = os.listdir(self.transcript_dir)   #List files in self.transcript_dir using os.listdir \n",
        "            transcript_names.sort()\n",
        "\n",
        "            assert len(mfcc_names) == len(transcript_names) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "            for i in range(len(mfcc_names)):\n",
        "                mfcc = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle=True)\n",
        "\n",
        "                # Augmenting the model by 20% more data which will be masked (Time and Frequency both)\n",
        "                if np.random.rand() <= 0.2:\n",
        "                    self.masked.append(self.mfcc_dir + mfcc_names[i])   # Copying the full Path name here\n",
        "                    T += mfcc.shape[0]  # Updating T\n",
        "\n",
        "                T += mfcc.shape[0]\n",
        "\n",
        "        partition.append('masked')\n",
        "\n",
        "        self.mfccs = np.zeros(shape = (T + 2*context, mfcc.shape[1]), dtype=np.float32)\n",
        "        self.transcripts = np.zeros(shape = (T, ), dtype=np.uint8)\n",
        "        cx, cy = context, 0\n",
        "\n",
        "\n",
        "        ############## Loading the files ###############\n",
        "        for idx in range(len(partition)):\n",
        "\n",
        "            self.mfcc_dir       = root + partition[idx] + '/mfcc/'   # Accessing the mfcc directory\n",
        "            self.transcript_dir = root + partition[idx] + '/transcript/'  # Accessing the transcript directory\n",
        "\n",
        "            if partition[idx] == 'masked':  # Loading the data names picked for masking\n",
        "                mfcc_names          = self.masked #List files in self.masked using os.listdir \n",
        "                mfcc_names.sort()\n",
        "\n",
        "                transcript_names    = self.masked #List files in self.masked using os.listdir \n",
        "                transcript_names.sort()\n",
        "\n",
        "            else:\n",
        "                mfcc_names          = os.listdir(self.mfcc_dir) #List files in self.mfcc_dir using os.listdir \n",
        "                mfcc_names.sort()\n",
        "\n",
        "                transcript_names    = os.listdir(self.transcript_dir)   #List files in self.transcript_dir using os.listdir \n",
        "                transcript_names.sort()\n",
        "\n",
        "            assert len(mfcc_names) == len(transcript_names) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "\n",
        "            for i in range(len(mfcc_names)):    #Iterating through mfccs and transcripts\n",
        "\n",
        "                if partition[idx] == 'masked':  # Doing Frequency Masking for the desired data\n",
        "\n",
        "                    mfcc        = np.load(mfcc_names[i], allow_pickle=True) #   Load a single mfcc\n",
        "                    mfcc        = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)   #   Do Cepstral Normalization of mfcc\n",
        "\n",
        "                    transcript  = np.load(transcript_names[i].replace('mfcc', 'transcript'), allow_pickle=True)   #   Load the corresponding transcript\n",
        "                    transcript = transcript[1:-1]   # Remove [SOS] and [EOS] from the transcript. Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "\n",
        "                    if np.random.rand() <= 0.5: # Frequency Masking\n",
        "                        num_freq_mask = np.random.randint(low = int(mfcc.shape[1] * 0.2), high = int(mfcc.shape[1] * 0.3)+1)   # Selcting Frequency Masking between 20% to 30%\n",
        "                        start_band = np.random.randint(low = 0, high = mfcc.shape[1] - num_freq_mask)\n",
        "                        mfcc[:, start_band : start_band+num_freq_mask] = 0      # Setting it to 0\n",
        "\n",
        "                    else:   # Time Masking\n",
        "                        mask_duration = np.random.randint(low = int(mfcc.shape[0] * 0.2), high = int(mfcc.shape[0] * 0.3)+1) # Selcting Masking between 20% to 30%\n",
        "                        mask_start = np.random.randint(low = 0, high = mfcc.shape[0] - mask_duration)\n",
        "                        mfcc[mask_start : mask_start+mask_duration, :] = 0      # Setting it to 0\n",
        "\n",
        "\n",
        "                else:   \n",
        "                    mfcc        = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle=True) #   Load a single mfcc\n",
        "                    mfcc        = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)   #   Do Cepstral Normalization of mfcc\n",
        "\n",
        "                    transcript  = np.load(self.transcript_dir + transcript_names[i], allow_pickle=True)   #   Load the corresponding transcript\n",
        "                    transcript = transcript[1:-1]   # Remove [SOS] and [EOS] from the transcript. Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "                \n",
        "                # Loading the data more efficiently\n",
        "                T_i = mfcc.shape[0] \n",
        "                self.mfccs[cx:cx+T_i] = mfcc\n",
        "                self.transcripts[cy:cy+T_i] = np.array([phonemes_dict[trans] for trans in transcript])\n",
        "\n",
        "                cx += T_i\n",
        "                cy += T_i\n",
        "\n",
        "            ### No need to concatenate since self.mfccs and self.transcripts are in desired shape\n",
        "\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = T\n",
        "\n",
        "        # No Need for Padding which is already taken care of\n",
        "\n",
        "        ### No need to map since self.transcripts already contains indices\n",
        "        #self.transcripts = np.array([self.phonemes.index(i) for i in self.transcripts])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind: ind+2*self.context+1]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten()\n",
        "\n",
        "        # Converting to Tensors\n",
        "        frames      = torch.FloatTensor(frames)\n",
        "        phonemes    = torch.tensor(self.transcripts[ind])       \n",
        "\n",
        "        return frames, phonemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FunemJGfD1Bi"
      },
      "outputs": [],
      "source": [
        "# Dataset class to only load validation Data\n",
        "\n",
        "class AudioDatasetEfficientVal(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, phonemes_dict = PHONEMES_DICT, context=0, partition= ['dev-clean']): # Feel free to add more arguments\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "    \n",
        "        ############ Efficient Data Loader ###############\n",
        "        ### Need to know the shape first\n",
        "        T = 0\n",
        "        for idx in range(len(partition)):\n",
        "        \n",
        "            self.mfcc_dir       = root + partition[idx] + '/mfcc/'   # Accessing the mfcc directory\n",
        "            self.transcript_dir = root + partition[idx] + '/transcript/'  # Accessing the transcript directory\n",
        "\n",
        "            mfcc_names          = os.listdir(self.mfcc_dir) #List files in self.mfcc_dir using os.listdir \n",
        "            mfcc_names.sort()\n",
        "\n",
        "            transcript_names    = os.listdir(self.transcript_dir)   #List files in self.transcript_dir using os.listdir \n",
        "            transcript_names.sort()\n",
        "\n",
        "            assert len(mfcc_names) == len(transcript_names) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "            for i in range(len(mfcc_names)):\n",
        "                mfcc = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle=True)\n",
        "                T += mfcc.shape[0]\n",
        "\n",
        "\n",
        "        self.mfccs = np.zeros(shape = (T + 2*context, mfcc.shape[1]), dtype=np.float32)\n",
        "        self.transcripts = np.zeros(shape = (T, ), dtype=np.uint8)\n",
        "        cx, cy = context, 0\n",
        "\n",
        "\n",
        "        ############## Loading the files ###############\n",
        "        for idx in range(len(partition)):\n",
        "        \n",
        "            self.mfcc_dir       = root + partition[idx] + '/mfcc/'   # Accessing the mfcc directory\n",
        "            self.transcript_dir = root + partition[idx] + '/transcript/'  # Accessing the transcript directory\n",
        "\n",
        "            mfcc_names          = os.listdir(self.mfcc_dir) #List files in self.mfcc_dir using os.listdir \n",
        "            mfcc_names.sort()\n",
        "\n",
        "            transcript_names    = os.listdir(self.transcript_dir)   #List files in self.transcript_dir using os.listdir \n",
        "            transcript_names.sort()\n",
        "\n",
        "            assert len(mfcc_names) == len(transcript_names) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "            for i in range(len(mfcc_names)):    #Iterating through mfccs and transcripts\n",
        "        \n",
        "                mfcc        = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle=True) #   Load a single mfcc\n",
        "                mfcc        = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)   #   Do Cepstral Normalization of mfcc\n",
        "\n",
        "                transcript  = np.load(self.transcript_dir + transcript_names[i], allow_pickle=True)   #   Load the corresponding transcript\n",
        "                transcript = transcript[1:-1]   # Remove [SOS] and [EOS] from the transcript. Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "            \n",
        "                # Loading the data more efficiently\n",
        "                T_i = mfcc.shape[0] \n",
        "                self.mfccs[cx:cx+T_i] = mfcc\n",
        "                self.transcripts[cy:cy+T_i] = np.array([phonemes_dict[trans] for trans in transcript])\n",
        "\n",
        "                cx += T_i\n",
        "                cy += T_i\n",
        "\n",
        "                    \n",
        "            ### No need to concatenate since self.mfccs and self.transcripts are in desired shape\n",
        "\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = T\n",
        "\n",
        "        # No Need for Padding which is already taken care of\n",
        "\n",
        "        ### No need to map since self.transcripts already contains indices\n",
        "        #self.transcripts = np.array([self.phonemes.index(i) for i in self.transcripts])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind: ind+2*self.context+1]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten()\n",
        "\n",
        "        # Converting to Tensors\n",
        "        frames      = torch.FloatTensor(frames)\n",
        "        phonemes    = torch.tensor(self.transcripts[ind])       \n",
        "\n",
        "        return frames, phonemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8KfVP39S6o7"
      },
      "outputs": [],
      "source": [
        "class AudioTestDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, root, context=0, partition= 'test-clean'):\n",
        "\n",
        "        self.context    = context\n",
        "        \n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = root + partition + '/mfcc/'\n",
        "\n",
        "        # TODO: List files in self.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = os.listdir(self.mfcc_dir)\n",
        "        mfcc_names.sort()\n",
        "\n",
        "        self.mfccs          = []\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)):\n",
        "    \n",
        "            mfcc        = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle=True) # Load a single mfcc\n",
        "            mfcc        = (mfcc - np.mean(mfcc, axis = 0))/np.std(mfcc, axis = 0)   #   Do Cepstral Normalization of mfcc\n",
        "\n",
        "            self.mfccs.append(mfcc)        \n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
        "        # Each transcript is of shape (T1+2) x 27, (T2+2) x 27 before removing [SOS] and [EOS]\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that \n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...) \n",
        "        self.mfccs      = np.concatenate(self.mfccs, axis = 0)\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # Context Padding\n",
        "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), mode = 'constant', constant_values = 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind: ind+2*self.context+1]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten()\n",
        "        # Converting to Tensors\n",
        "        frames      = torch.FloatTensor(frames)   \n",
        "\n",
        "        return frames\n",
        "    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
        "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mllfOhiuceq_"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WE7tsinAuLNy"
      },
      "source": [
        "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMjC7dNpvQLe"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'epochs'        : 38,\n",
        "    'batch_size'    : 2048,\n",
        "    'context'       : 20,\n",
        "    'learning_rate' : 1e-3,\n",
        "    'architecture'  : 'HighCutoff',\n",
        "    'best_accuracy' : 0,\n",
        "    'dropout'       : 0.20,\n",
        "    'convergence'   : 0\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYWwvZOvt6wz"
      },
      "outputs": [],
      "source": [
        "# train_data_efficient = AudioDatasetEfficientVal(root = '/content/data/11-785-s23-hw1p2/', phonemes = PHONEMES, phonemes_dict = PHONEMES_DICT, \n",
        "#                         context = config['context'], partition = ['dev-clean']) \n",
        "\n",
        "# train_data = AudioDataset(root = '/content/data/11-785-s23-hw1p2/', phonemes = PHONEMES,\n",
        "#                         context = config['context'], partition = ['dev-clean']) \n",
        "\n",
        "# print(train_data_efficient.__len__())\n",
        "# print(train_data.__len__())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1IbjjWVScqtD"
      },
      "source": [
        "## Create Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xi7V8x8W9z4"
      },
      "outputs": [],
      "source": [
        "# Function to create the datasets\n",
        "train_data = AudioDatasetEfficientTrain(root = '/content/data/11-785-s23-hw1p2/', phonemes = PHONEMES, phonemes_dict = PHONEMES_DICT,\n",
        "                        context = config['context'], partition = ['train-clean-360', 'train-clean-100']) \n",
        "\n",
        "# Create a dataset object using the AudioDataset class for the validation data \n",
        "val_data = AudioDatasetEfficientVal(root = '/content/data/11-785-s23-hw1p2/', phonemes = PHONEMES, phonemes_dict = PHONEMES_DICT,\n",
        "                        context = config['context'], partition = ['dev-clean'])  \n",
        "\n",
        "# Create a dataset object using the AudioTestDataset class for the test data \n",
        "test_data = AudioTestDataset(root = '/content/data/11-785-s23-hw1p2/', context = config['context'], \n",
        "                             partition = 'test-clean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "55c56c39-4d3d-423f-c1b1-213b5e24ebe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size     :  2048\n",
            "Context        :  20\n",
            "Input size     :  1107\n",
            "Output symbols :  40\n",
            "Train dataset samples = 199550088, batches = 97437\n",
            "Validation dataset samples = 1928204, batches = 942\n",
            "Test dataset samples = 1934138, batches = 945\n",
            "torch.Size([2048, 1107]) torch.Size([2048])\n"
          ]
        }
      ],
      "source": [
        "# Define dataloaders for train, val and test datasets\n",
        "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
        "# We shuffle train dataloader but not val & test dataloader. Why?\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 6,\n",
        "    batch_size  = config['batch_size'], \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data, \n",
        "    num_workers = 2, \n",
        "    batch_size  = config['batch_size'], \n",
        "    pin_memory  = True, \n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Batch size     : \", config['batch_size'])\n",
        "print(\"Context        : \", config['context'])\n",
        "print(\"Input size     : \", (2*config['context']+1)*27)\n",
        "print(\"Output symbols : \", len(PHONEMES))\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
        "\n",
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    frames, phoneme = data\n",
        "    print(frames.shape, phoneme.shape)\n",
        "    break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjwve20JRJ2"
      },
      "source": [
        "# Network Architecture\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJzT-mRw6iy"
      },
      "source": [
        "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPWq2vWiSabf"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZrqiWHqSav-"
      },
      "outputs": [],
      "source": [
        "# This architecture will make you cross the very low cutoff\n",
        "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, dropout):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, input_size*2),\n",
        "            torch.nn.BatchNorm1d(input_size*2),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "             \n",
        "            torch.nn.Linear(input_size*2, 2048), \n",
        "            torch.nn.BatchNorm1d(2048),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "\n",
        "            torch.nn.Linear(2048, 2048),\n",
        "            torch.nn.BatchNorm1d(2048),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "\n",
        "            torch.nn.Linear(2048, 2048),\n",
        "            torch.nn.BatchNorm1d(2048),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "\n",
        "            torch.nn.Linear(2048, 1024),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            \n",
        "            torch.nn.Linear(1024, 1024),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "\n",
        "            torch.nn.Linear(1024, 512),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout/2),\n",
        "\n",
        "            torch.nn.Linear(512, 512),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.GELU(),\n",
        "        \n",
        "            torch.nn.Linear(512, output_size)\n",
        "        )      \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qCAJQzQIVSD8",
        "outputId": "8a623432-4a9e-4dc1-b224-5bd2576aaf9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================\n",
            "                         Kernel Shape  Output Shape     Params  Mult-Adds\n",
            "Layer                                                                    \n",
            "0_model.Linear_0         [1107, 2214]  [2048, 2214]  2.453112M  2.450898M\n",
            "1_model.BatchNorm1d_1          [2214]  [2048, 2214]     4.428k     2.214k\n",
            "2_model.GELU_2                      -  [2048, 2214]          -          -\n",
            "3_model.Dropout_3                   -  [2048, 2214]          -          -\n",
            "4_model.Linear_4         [2214, 2048]  [2048, 2048]   4.53632M  4.534272M\n",
            "5_model.BatchNorm1d_5          [2048]  [2048, 2048]     4.096k     2.048k\n",
            "6_model.GELU_6                      -  [2048, 2048]          -          -\n",
            "7_model.Dropout_7                   -  [2048, 2048]          -          -\n",
            "8_model.Linear_8         [2048, 2048]  [2048, 2048]  4.196352M  4.194304M\n",
            "9_model.BatchNorm1d_9          [2048]  [2048, 2048]     4.096k     2.048k\n",
            "10_model.GELU_10                    -  [2048, 2048]          -          -\n",
            "11_model.Dropout_11                 -  [2048, 2048]          -          -\n",
            "12_model.Linear_12       [2048, 2048]  [2048, 2048]  4.196352M  4.194304M\n",
            "13_model.BatchNorm1d_13        [2048]  [2048, 2048]     4.096k     2.048k\n",
            "14_model.GELU_14                    -  [2048, 2048]          -          -\n",
            "15_model.Dropout_15                 -  [2048, 2048]          -          -\n",
            "16_model.Linear_16       [2048, 1024]  [2048, 1024]  2.098176M  2.097152M\n",
            "17_model.BatchNorm1d_17        [1024]  [2048, 1024]     2.048k     1.024k\n",
            "18_model.GELU_18                    -  [2048, 1024]          -          -\n",
            "19_model.Dropout_19                 -  [2048, 1024]          -          -\n",
            "20_model.Linear_20       [1024, 1024]  [2048, 1024]    1.0496M  1.048576M\n",
            "21_model.BatchNorm1d_21        [1024]  [2048, 1024]     2.048k     1.024k\n",
            "22_model.GELU_22                    -  [2048, 1024]          -          -\n",
            "23_model.Dropout_23                 -  [2048, 1024]          -          -\n",
            "24_model.Linear_24        [1024, 512]   [2048, 512]     524.8k   524.288k\n",
            "25_model.BatchNorm1d_25         [512]   [2048, 512]     1.024k      512.0\n",
            "26_model.GELU_26                    -   [2048, 512]          -          -\n",
            "27_model.Dropout_27                 -   [2048, 512]          -          -\n",
            "28_model.Linear_28         [512, 512]   [2048, 512]   262.656k   262.144k\n",
            "29_model.BatchNorm1d_29         [512]   [2048, 512]     1.024k      512.0\n",
            "30_model.GELU_30                    -   [2048, 512]          -          -\n",
            "31_model.Linear_31          [512, 40]    [2048, 40]     20.52k     20.48k\n",
            "-------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          19.360748M\n",
            "Trainable params      19.360748M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             19.337848M\n",
            "=========================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-aa3c0a06-69ef-4a8d-857c-5d38c7dd524a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_model.Linear_0</th>\n",
              "      <td>[1107, 2214]</td>\n",
              "      <td>[2048, 2214]</td>\n",
              "      <td>2453112.0</td>\n",
              "      <td>2450898.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_model.BatchNorm1d_1</th>\n",
              "      <td>[2214]</td>\n",
              "      <td>[2048, 2214]</td>\n",
              "      <td>4428.0</td>\n",
              "      <td>2214.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_model.GELU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2214]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_model.Dropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2214]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_model.Linear_4</th>\n",
              "      <td>[2214, 2048]</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>4536320.0</td>\n",
              "      <td>4534272.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_model.BatchNorm1d_5</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_model.GELU_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_model.Dropout_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_model.Linear_8</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_model.BatchNorm1d_9</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_model.GELU_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_model.Dropout_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_model.Linear_12</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_model.BatchNorm1d_13</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_model.GELU_14</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_model.Dropout_15</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_model.Linear_16</th>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>2098176.0</td>\n",
              "      <td>2097152.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_model.BatchNorm1d_17</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_model.GELU_18</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_model.Dropout_19</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_model.Linear_20</th>\n",
              "      <td>[1024, 1024]</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>1049600.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_model.BatchNorm1d_21</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_model.GELU_22</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_model.Dropout_23</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_model.Linear_24</th>\n",
              "      <td>[1024, 512]</td>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>524800.0</td>\n",
              "      <td>524288.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_model.BatchNorm1d_25</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_model.GELU_26</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_model.Dropout_27</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_model.Linear_28</th>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>262656.0</td>\n",
              "      <td>262144.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_model.BatchNorm1d_29</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30_model.GELU_30</th>\n",
              "      <td>-</td>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31_model.Linear_31</th>\n",
              "      <td>[512, 40]</td>\n",
              "      <td>[2048, 40]</td>\n",
              "      <td>20520.0</td>\n",
              "      <td>20480.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa3c0a06-69ef-4a8d-857c-5d38c7dd524a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa3c0a06-69ef-4a8d-857c-5d38c7dd524a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa3c0a06-69ef-4a8d-857c-5d38c7dd524a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         Kernel Shape  Output Shape     Params  Mult-Adds\n",
              "Layer                                                                    \n",
              "0_model.Linear_0         [1107, 2214]  [2048, 2214]  2453112.0  2450898.0\n",
              "1_model.BatchNorm1d_1          [2214]  [2048, 2214]     4428.0     2214.0\n",
              "2_model.GELU_2                      -  [2048, 2214]        NaN        NaN\n",
              "3_model.Dropout_3                   -  [2048, 2214]        NaN        NaN\n",
              "4_model.Linear_4         [2214, 2048]  [2048, 2048]  4536320.0  4534272.0\n",
              "5_model.BatchNorm1d_5          [2048]  [2048, 2048]     4096.0     2048.0\n",
              "6_model.GELU_6                      -  [2048, 2048]        NaN        NaN\n",
              "7_model.Dropout_7                   -  [2048, 2048]        NaN        NaN\n",
              "8_model.Linear_8         [2048, 2048]  [2048, 2048]  4196352.0  4194304.0\n",
              "9_model.BatchNorm1d_9          [2048]  [2048, 2048]     4096.0     2048.0\n",
              "10_model.GELU_10                    -  [2048, 2048]        NaN        NaN\n",
              "11_model.Dropout_11                 -  [2048, 2048]        NaN        NaN\n",
              "12_model.Linear_12       [2048, 2048]  [2048, 2048]  4196352.0  4194304.0\n",
              "13_model.BatchNorm1d_13        [2048]  [2048, 2048]     4096.0     2048.0\n",
              "14_model.GELU_14                    -  [2048, 2048]        NaN        NaN\n",
              "15_model.Dropout_15                 -  [2048, 2048]        NaN        NaN\n",
              "16_model.Linear_16       [2048, 1024]  [2048, 1024]  2098176.0  2097152.0\n",
              "17_model.BatchNorm1d_17        [1024]  [2048, 1024]     2048.0     1024.0\n",
              "18_model.GELU_18                    -  [2048, 1024]        NaN        NaN\n",
              "19_model.Dropout_19                 -  [2048, 1024]        NaN        NaN\n",
              "20_model.Linear_20       [1024, 1024]  [2048, 1024]  1049600.0  1048576.0\n",
              "21_model.BatchNorm1d_21        [1024]  [2048, 1024]     2048.0     1024.0\n",
              "22_model.GELU_22                    -  [2048, 1024]        NaN        NaN\n",
              "23_model.Dropout_23                 -  [2048, 1024]        NaN        NaN\n",
              "24_model.Linear_24        [1024, 512]   [2048, 512]   524800.0   524288.0\n",
              "25_model.BatchNorm1d_25         [512]   [2048, 512]     1024.0      512.0\n",
              "26_model.GELU_26                    -   [2048, 512]        NaN        NaN\n",
              "27_model.Dropout_27                 -   [2048, 512]        NaN        NaN\n",
              "28_model.Linear_28         [512, 512]   [2048, 512]   262656.0   262144.0\n",
              "29_model.BatchNorm1d_29         [512]   [2048, 512]     1024.0      512.0\n",
              "30_model.GELU_30                    -   [2048, 512]        NaN        NaN\n",
              "31_model.Linear_31          [512, 40]    [2048, 40]    20520.0    20480.0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_SIZE  = (2*config['context'] + 1) * 27 # Why is this the case?\n",
        "model       = Network(INPUT_SIZE, len(train_data.phonemes), 0.2).to(device)\n",
        "summary(model, frames.to(device))\n",
        "# Check number of parameters of your network\n",
        "# Remember, you are limited to 20 million parameters for HW1 (including ensembles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPwdOHSq4RPM",
        "outputId": "0040cbbc-ad68-4f19-fabc-4f8c58b453c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Network(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=1107, out_features=2214, bias=True)\n",
              "    (1): BatchNorm1d(2214, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=2214, out_features=2048, bias=True)\n",
              "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): GELU(approximate='none')\n",
              "    (7): Dropout(p=0.2, inplace=False)\n",
              "    (8): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): GELU(approximate='none')\n",
              "    (11): Dropout(p=0.2, inplace=False)\n",
              "    (12): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (13): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): GELU(approximate='none')\n",
              "    (15): Dropout(p=0.2, inplace=False)\n",
              "    (16): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (17): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): GELU(approximate='none')\n",
              "    (19): Dropout(p=0.2, inplace=False)\n",
              "    (20): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (21): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): GELU(approximate='none')\n",
              "    (23): Dropout(p=0.2, inplace=False)\n",
              "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (25): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): GELU(approximate='none')\n",
              "    (27): Dropout(p=0.1, inplace=False)\n",
              "    (28): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (29): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (30): GELU(approximate='none')\n",
              "    (31): Linear(in_features=512, out_features=40, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initializing weights\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HejoSXe3vMVU"
      },
      "source": [
        "# Define Model, Loss Function and Optimizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhGBH7-xxth"
      },
      "source": [
        "Here we define the model, loss function, optimizer and optionally a learning rate scheduler. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UROGEVJevKD-"
      },
      "outputs": [],
      "source": [
        "############################ Loss Criterion ############################\n",
        "criterion = torch.nn.CrossEntropyLoss() # We use CE because the task is multi-class classification \n",
        "############################ Loss Criterion ############################\n",
        "\n",
        "\n",
        "############################### Optimizer ##############################\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = config['learning_rate'], weight_decay = config['learning_rate']*10)\n",
        "############################### Optimizer ##############################\n",
        "\n",
        "\n",
        "############################### Scheduler ##############################\n",
        "#scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lambda epoch: 0.98**epoch, verbose=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor = 0.5, patience = 2, threshold=0.25, verbose = True, min_lr = 1e-8)\n",
        "############################### Scheduler ##############################\n",
        "\n",
        "# Is your training time very high? \n",
        "# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it \n",
        "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1JgeNhx4x2-P"
      },
      "source": [
        "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XblOHEVtKab2"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "### Defining scaler\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wjPz7DHqKcL"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "    \n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "        \n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        with torch.autocast(device_type='cuda', dtype = torch.float16):\n",
        "          ### Forward Propagation\n",
        "          logits  = model(frames)\n",
        "          ### Loss Calculation\n",
        "          loss    = criterion(logits, phonemes)\n",
        "\n",
        "        ### Backward Propagation\n",
        "        #loss.backward() \n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        ### Gradient Descent\n",
        "        scaler.step(optimizer)       \n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))), \n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "        scaler.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "  \n",
        "    batch_bar.close()\n",
        "    tloss   /= len(dataloader)\n",
        "    tacc    /= len(dataloader)\n",
        "\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode(): \n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "        \n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))), \n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "    \n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(dataloader)\n",
        "    vacc    /= len(dataloader)\n",
        "\n",
        "    return vloss, vacc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMd_XxPku5qp"
      },
      "source": [
        "# Weights and Biases Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIbhR1wwbgI"
      },
      "source": [
        "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb. \n",
        "\n",
        "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDYx5VEu6qI",
        "outputId": "5c373508-2fec-43b5-8a36-9a0ba55555ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msethilakshay13\u001b[0m (\u001b[33mhighcutoff\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"d8c7e26b1505e86567dd118f06a50f03dbfb28d6\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "xvUnYd3Bw2up",
        "outputId": "1ec5c69f-cc97-4804-a50c-60a41b0336d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230216_100433-8dfxovgb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/highcutoff/Lakshay/runs/8dfxovgb' target=\"_blank\">Wunderwaffe_Model</a></strong> to <a href='https://wandb.ai/highcutoff/Lakshay' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/highcutoff/Lakshay' target=\"_blank\">https://wandb.ai/highcutoff/Lakshay</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/highcutoff/Lakshay/runs/8dfxovgb' target=\"_blank\">https://wandb.ai/highcutoff/Lakshay/runs/8dfxovgb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"Wunderwaffe_Model\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"Lakshay\", ### Project should be created in your wandb account \n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wft15E_IxYFi",
        "outputId": "28270dde-46b4-4399-eee9-97f4754f7c12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/wandb/run-20230216_100433-8dfxovgb/files/Wunderwaffe_Model']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ### Save your model architecture as a string with str(model) \n",
        "model_arch  = str(model)\n",
        "\n",
        "### Save it in a txt file \n",
        "arch_file   = open(\"Wunderwaffe_Model\", \"w\")\n",
        "file_write  = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('Wunderwaffe_Model')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nclx_04fu7Dd"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLMWfEpyGOB"
      },
      "source": [
        "Now, it is time to finally run your ablations! Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "96e3a061e5c64b889a653ccfc2959a3e",
            "d90af819bfcc4b47bc8d5a4eae62f6aa",
            "aa5671edb98743149aef5585e486873d",
            "a37c789a833f4db386f24e837eadbc88",
            "376d76ffe1bd4160a7817efacf4a2bd7",
            "db70bfab1b0c4c74b84a785b1af98f6b",
            "f5509a0be5d94575bd899a23275afa45",
            "3e272e181d8b4b2e8a90116acb32d599",
            "d329fcf755274138a52459c122e25b31",
            "9b54f02d05a24ed1b08b8be76b59757d",
            "9ff9964973b040a692adfc0e00450ed8"
          ]
        },
        "id": "4o0jQfvsYq42",
        "outputId": "a2e8fb48-92ec-4d19-afdc-d88d3364506d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/38\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96e3a061e5c64b889a653ccfc2959a3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train:   0%|          | 0/97437 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "#val_acc_prev = 0\n",
        "\n",
        "# Training till convergence\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc       = eval(model, val_loader)\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
        "\n",
        "    ### Log metrics at each epoch in your run \n",
        "    # Optionally, you can log at each batch inside train/eval functions \n",
        "    # (explore wandb documentation/wandb recitation)\n",
        "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss, \n",
        "               'val_acc': val_acc*100, 'val_loss': val_loss, 'lr': curr_lr})\n",
        "    \n",
        "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
        "    torch.save({'epoch': (epoch+12), \n",
        "                'model_state_dict': model.state_dict(), \n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc},\n",
        "               ('/content/Checkpoints/LakshayModel_CheckPoint_Epoch_'))\n",
        "    \n",
        "    ### Convergence check: break if model has converged\n",
        "    # if (abs(val_acc - val_acc_prev)) < config['convergence']:\n",
        "    #     print(\"\\n\")\n",
        "    #     print(\"################################################\")\n",
        "    #     print(\"Breaking due to convergence at epoch {}\".format(epoch))\n",
        "    #     print(\"################################################\")\n",
        "    #     print(\"\\n\")\n",
        "    #     break\n",
        "\n",
        "    # val_acc_prev = val_acc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXwf5YUo_4A"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1hSFYLpJvH"
      },
      "source": [
        "Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    ### What you call for model to perform inference?\n",
        "    model.eval() # TODO train or eval?\n",
        "\n",
        "    ### List to store predicted phonemes of test data\n",
        "    test_predictions = []\n",
        "\n",
        "    ### Which mode do you need to avoid gradients?\n",
        "    with torch.inference_mode(): # TODO\n",
        "\n",
        "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
        "\n",
        "            mfccs   = mfccs.to(device)             \n",
        "            \n",
        "            logits  = model(mfccs)\n",
        "\n",
        "            ### Get most likely predicted phoneme with argmax\n",
        "            predicted_phonemes = (torch.argmax(logits, dim = 1)).tolist()\n",
        "\n",
        "            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval \n",
        "            test_predictions += (PHONEMES[val] for val in predicted_phonemes)\n",
        "\n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "96483275bf57407684b4bf65c5c5d20c",
            "2221f878e0ac4e5db1b2367fffff7c81",
            "0f3c17907e594049be3bf31d6881c80f",
            "07422964e4594fe78dd233d124448810",
            "61a77d55ddac47acb2c8106c2b4a7c49",
            "7a0c402866564a4582aa5ba1fa8b10e5",
            "b13de677f8a344358823a93b3b107e4d",
            "ab2d16d9bdc44938af4251e1748b4df0",
            "c62bbb333a74491ab7e36061394d0205",
            "5fbbffed8bc4474a8745a53f3afb3062",
            "b1cf73b0f54b4cd2bacb20ab12ea49c2"
          ]
        },
        "id": "wG9v6Xmxu7wp",
        "outputId": "215df4ea-28a8-4b26-c219-0433d5b25bc3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96483275bf57407684b4bf65c5c5d20c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/945 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "predictions = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "### Create CSV file with predictions\n",
        "with open(\"./submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(predictions)):\n",
        "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjcammuCxMKN",
        "outputId": "57066ea4-1aeb-4301-cf5a-34ac171fad39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 19.3M/19.3M [00:02<00:00, 9.16MB/s]\n",
            "Successfully submitted to Frame-Level Speech Recognition"
          ]
        }
      ],
      "source": [
        "### Submit to kaggle competition using kaggle API (Uncomment below to use)\n",
        "\n",
        "!kaggle competitions submit -c 11-785-s23-hw1p2 -f ./submission.csv -m \"Test Submission\"\n",
        "\n",
        "### However, its always safer to download the csv file and then upload to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "csko-X9p7xvX",
        "outputId": "cc8b34fb-cfb3-410d-f8f9-66501e92e642"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>valid_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.001</td></tr><tr><td>train_acc</td><td>70.63945</td></tr><tr><td>train_loss</td><td>0.90753</td></tr><tr><td>val_acc</td><td>69.72801</td></tr><tr><td>valid_loss</td><td>0.93224</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">first-run</strong> at: <a href='https://wandb.ai/sethilakshay13/hw1p2/runs/7lfv8hv4' target=\"_blank\">https://wandb.ai/sethilakshay13/hw1p2/runs/7lfv8hv4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230212_000913-7lfv8hv4/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Finish your wandb run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK9H0DjcDlYc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07422964e4594fe78dd233d124448810": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fbbffed8bc4474a8745a53f3afb3062",
            "placeholder": "​",
            "style": "IPY_MODEL_b1cf73b0f54b4cd2bacb20ab12ea49c2",
            "value": " 945/945 [00:14&lt;00:00, 81.44it/s]"
          }
        },
        "0f3c17907e594049be3bf31d6881c80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab2d16d9bdc44938af4251e1748b4df0",
            "max": 945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c62bbb333a74491ab7e36061394d0205",
            "value": 945
          }
        },
        "2221f878e0ac4e5db1b2367fffff7c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a0c402866564a4582aa5ba1fa8b10e5",
            "placeholder": "​",
            "style": "IPY_MODEL_b13de677f8a344358823a93b3b107e4d",
            "value": "100%"
          }
        },
        "376d76ffe1bd4160a7817efacf4a2bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "3e272e181d8b4b2e8a90116acb32d599": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fbbffed8bc4474a8745a53f3afb3062": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a77d55ddac47acb2c8106c2b4a7c49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0c402866564a4582aa5ba1fa8b10e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96483275bf57407684b4bf65c5c5d20c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2221f878e0ac4e5db1b2367fffff7c81",
              "IPY_MODEL_0f3c17907e594049be3bf31d6881c80f",
              "IPY_MODEL_07422964e4594fe78dd233d124448810"
            ],
            "layout": "IPY_MODEL_61a77d55ddac47acb2c8106c2b4a7c49"
          }
        },
        "96e3a061e5c64b889a653ccfc2959a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d90af819bfcc4b47bc8d5a4eae62f6aa",
              "IPY_MODEL_aa5671edb98743149aef5585e486873d",
              "IPY_MODEL_a37c789a833f4db386f24e837eadbc88"
            ],
            "layout": "IPY_MODEL_376d76ffe1bd4160a7817efacf4a2bd7"
          }
        },
        "9b54f02d05a24ed1b08b8be76b59757d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff9964973b040a692adfc0e00450ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a37c789a833f4db386f24e837eadbc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b54f02d05a24ed1b08b8be76b59757d",
            "placeholder": "​",
            "style": "IPY_MODEL_9ff9964973b040a692adfc0e00450ed8",
            "value": " 0/97437 [00:00&lt;?, ?it/s]"
          }
        },
        "aa5671edb98743149aef5585e486873d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e272e181d8b4b2e8a90116acb32d599",
            "max": 97437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d329fcf755274138a52459c122e25b31",
            "value": 0
          }
        },
        "ab2d16d9bdc44938af4251e1748b4df0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13de677f8a344358823a93b3b107e4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1cf73b0f54b4cd2bacb20ab12ea49c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c62bbb333a74491ab7e36061394d0205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d329fcf755274138a52459c122e25b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d90af819bfcc4b47bc8d5a4eae62f6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db70bfab1b0c4c74b84a785b1af98f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_f5509a0be5d94575bd899a23275afa45",
            "value": "Train:   0%"
          }
        },
        "db70bfab1b0c4c74b84a785b1af98f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5509a0be5d94575bd899a23275afa45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
